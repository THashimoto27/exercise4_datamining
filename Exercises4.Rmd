---
title: "Exercise 4"
author: "Takehiro Hasimoto (UT EID - TH33985), Avijit Mallik (UT EID - AM99484), Arindam
  Chatterjee (UT EID - AC83995)"
date: "`r Sys.Date()`"
output:
 md_document
---

```{r, include=FALSE,eval=FALSE}
options(tinytex.verbose = TRUE)
options(dplyr.summarise.inform = FALSE)
```

```{r setup, include=FALSE,eval=FALSE}
library(magrittr)
library(dplyr)
library(tidyverse) 
library(sjmisc)
library(ggplot2)
library(reshape2)
library(gapminder)
library(mosaic)
library(extraDistr)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(rsample)
library(lubridate)
library(olsrr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(gamlr)
```

# 1) Clustering and PCA

## 1-1 Procedures


## 1-2 Result (White and Red)

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide',eval=FALSE}
wine <- read.csv("wine.csv")

## PCA
pca_result <- prcomp(wine[-13], scale. = TRUE)
wine_types <- as.factor(wine$color)
levels(wine_types) <- c("Red", "White")

# Prepare the PCA data for ggplot2
pca_data <- as.data.frame(pca_result$x[, 1:2])
colnames(pca_data) <- c("PC1", "PC2")
pca_data$wine_type <- wine_types

# Visualize PCA results
ggplot(pca_data, aes(x = PC1, y = PC2, color = wine_type)) +
  geom_point(alpha = 0.7) +
  theme_bw() +
  labs(title = "PCA Plot of Wine Data", x = "PC1", y = "PC2") +
  theme(legend.title = element_text(size = 12), legend.text = element_text(size = 10))
ggsave("./fig/1pca.png")

## Clustering
# K-means
# we wanna to identify them into two clusters and so use K-means at 2.
X = wine[-13]
X = scale(X, center=TRUE, scale=TRUE)

wine_kmean = kmeans(X, 2, nstart=25)

# Red and White (Actual data)
ggplot(wine) + 
  geom_point(aes(pH,fixed.acidity, color=factor(color)))
ggsave("./fig/1act1.png")

# Good identification
ggplot(wine) + 
  geom_point(aes(pH,fixed.acidity, color=factor(wine_kmean$cluster)))+
    labs(title="Plot of k-means(good identification)")
ggsave("./fig/1km1.png")


## Hierarchical clustering
wine_distance_matrix = dist(X, method='euclidean')

hier_wine = hclust(wine_distance_matrix, method='single')
hier_clust_sing = cutree(hier_wine, k=2)
hier_wine = hclust(wine_distance_matrix, method='complete')
hier_clust_comp = cutree(hier_wine, k=2)
hier_wine = hclust(wine_distance_matrix, method='average')
hier_clust_avg = cutree(hier_wine, k=2)

# H-clustering -> bad
ggplot(wine) + 
  geom_point(aes(pH,fixed.acidity, color=factor(hier_clust_sing)))+
    labs(title="Plot of HC(single)",
        x ="pH", y = "fixed.acidity")
ggsave("./fig/1sing.png")
ggplot(wine) + 
  geom_point(aes(pH,fixed.acidity, color=factor(hier_clust_comp)))+
    labs(title="Plot of HC(average)",
        x ="pH", y = "fixed.acidity")
ggsave("./fig/1comp.png")
ggplot(wine) + 
  geom_point(aes(pH,fixed.acidity, color=factor(hier_clust_avg)))+
    labs(title="Plot of HC(complete)",
        x ="pH", y = "fixed.acidity")
ggsave("./fig/1avg.png")
```

### PCA

First, we shows the result of the PCA method, which looks like it can be good identification. 

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1pca.png")
```

### K-means

Second, we shows the result of the K-means method (K=2). This is the graph of the actual data.
```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1act2.png")
```

And, this is the k-means graph that looks like being able to be the same as the actual data. So, we can think this method can distinguish data into two parts(x-axis=pH, y-axis=fixed.acidity) because these factors are different between white and red wines.

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1km1.png")
```

### Hierarchical clustering

Third, we used Hierarchical clustering with the minimum linkage of "single", "complete" and "average". However, all of them looks like bad identification. Probably because Hierarchical clustering identify data into two parts step by step and so in the case of white and red wine(they looks like almost same characteristics) this method doesn't work well with only unsupervised technique.

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1sing.png")
knitr::include_graphics("./fig/1comp.png")
knitr::include_graphics("./fig/1avg.png")
```

## 1-3 Result (Quality)

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide',eval=FALSE}
## PCA
wine2 <- wine %>% mutate(color = case_when(
                color == "red" ~ 1,
                color == "white" ~ 2
))

pca_result <- prcomp(wine2[-12], scale. = TRUE)
wine_types <- as.factor(wine$quality)
levels(wine_types) <- c("3", "4","5","6","7","8","9")

# Prepare the PCA data for ggplot2
pca_data <- as.data.frame(pca_result$x[, 1:2])
colnames(pca_data) <- c("PC1", "PC2")
pca_data$wine_type <- wine_types

# Visualize PCA results
ggplot(pca_data, aes(x = PC1, y = PC2, color = wine_type)) +
  geom_point(alpha = 0.7) +
  theme_bw() +
  labs(title = "PCA Plot of Wine Data", x = "PC1", y = "PC2") +
  theme(legend.title = element_text(size = 12), legend.text = element_text(size = 10))
ggsave("./fig/1pca2.png")

## Clustering Bad
# K-means
# we wanna to identify them into two clusters and so use K-means at 2.
X = wine2[-12]
X = scale(X, center=TRUE, scale=TRUE)
wine_kmean = kmeans(X, 7, nstart=25)

# Quality (Actual data) Bad
ggplot(wine2) + 
  geom_point(aes(pH,total.sulfur.dioxide, color=factor(quality)))
ggsave("./fig/1act2.png")

# Bad
ggplot(wine2) + 
  geom_point(aes(pH,total.sulfur.dioxide, color=factor(wine_kmean$cluster)))+
    labs(title="Plot of k-means")
ggsave("./fig/1km2.png")
```


### PCA

We cannot distinguish the quality of the wine in PCA.

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1pca2.png")
```

### Clustering

At the actual data, We cannot distinguish the quality of the wine well.

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1act2.png")
```

Therefore, we cannot judge that this clustering did work well.

```{r, echo=FALSE,out.width ="70%", out.height = "50%",fig.align='center',fig.show="hold"}
knitr::include_graphics("./fig/1km2.png")
```

## 1-4 Conclusion (Answers)

In conclusion, the best technique that makes sense to me was **"PCA"** in our analysis because it can identify data into two parts automatically. The second one is "K-mean" because if we set adequate x-axis and y-axis, we can identify data well.

However, we cannot distinguish the quality of the wine well as we showed above, probably because we need more the number of data on wine or these characteristics on wine in data does not relate to the quality.

# 2)